# H∆∞·ªõng D·∫´n Tri·ªÉn Khai v√† Tri·ªÉn Khai D·ª± √Ån AdnLog API

## T·ªïng Quan B√†i To√°n
**Y√™u c·∫ßu**: X√¢y d·ª±ng server API tr·∫£ v·ªÅ s·ªë l∆∞·ª£ng user view/click cho campaign/banner theo kho·∫£ng th·ªùi gian, v·ªõi th·ªùi gian ph·∫£n h·ªìi < 1 ph√∫t.

**ƒê·∫ßu v√†o**: 
- Log qu·∫£ng c√°o v·ªõi c√°c tr∆∞·ªùng: `guid` (ID ng∆∞·ªùi d√πng), `campaignId`, `bannerId`, `click_or_view` (false=view, true=click), `time_create`.

**ƒê·∫ßu ra**: 
- Endpoint API tr·∫£ v·ªÅ s·ªë l∆∞·ª£ng user unique ƒë√£ view/click campaign/banner trong kho·∫£ng th·ªùi gian ƒë√£ cho.

---

## Ki·∫øn Tr√∫c Gi·∫£i Ph√°p
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Flask API     ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  Apache Spark   ‚îÇ‚îÄ‚îÄ‚ñ∂‚îÇ Ngu·ªìn D·ªØ Li·ªáu   ‚îÇ
‚îÇ   (REST API)    ‚îÇ    ‚îÇ  (X·ª≠ l√Ω)        ‚îÇ    ‚îÇ (HDFS/M·∫´u)      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
        ‚îÇ                       ‚îÇ                       ‚îÇ
        ‚îÇ                       ‚îÇ                       ‚îÇ
    Gunicorn              B·ªô Nh·ªõ Cache            File Parquet
   (S·∫£n xu·∫•t)           (Hi·ªáu su·∫•t)            (L∆∞u tr·ªØ)
```

**Chi·∫øn l∆∞·ª£c ch√≠nh**: Cache to√†n b·ªô d·ªØ li·ªáu v√†o b·ªô nh·ªõ ‚Üí Th·ª±c hi·ªán truy v·∫•n t·ª´ b·ªô nh·ªõ thay v√¨ ƒëƒ©a ‚Üí Th·ªùi gian ph·∫£n h·ªìi < 1 gi√¢y.

---

## Chi Ti·∫øt Tri·ªÉn Khai

### **1. C√°c File C·ªët L√µi c·ªßa ·ª®ng D·ª•ng**

#### **app.py - ·ª®ng d·ª•ng Flask Ch√≠nh**
```python
from flask import Flask, request, jsonify
from datetime import datetime
from spark_session import get_spark
from data_processor import AdnLogProcessor

app = Flask(__name__)
spark = None
processor = None

def init_app():
    """Kh·ªüi t·∫°o Spark session v√† t·∫£i d·ªØ li·ªáu"""
    global spark, processor
    try:
        # B∆∞·ªõc 1: Kh·ªüi t·∫°o Spark session
        spark = get_spark()

        # B∆∞·ªõc 2: Kh·ªüi t·∫°o b·ªô x·ª≠ l√Ω d·ªØ li·ªáu
        processor = AdnLogProcessor(spark)

        # B∆∞·ªõc 3: T·∫£i v√† cache d·ªØ li·ªáu
        if processor.load_and_cache_data():
            return True
        return False
    except Exception as e:
        logger.error(f"L·ªói kh·ªüi t·∫°o: {str(e)}")
        return False

@app.route("/query", methods=["GET"])
def query_user_count():
    """Endpoint ch√≠nh ƒë·ªÉ truy v·∫•n"""
    start_time = datetime.now()

    # L·∫•y tham s·ªë
    id_type = request.args.get("id_type")
    target_id = request.args.get("id")
    mode = request.args.get("mode")
    from_date = request.args.get("from")
    to_date = request.args.get("to")

    # Ki·ªÉm tra tham s·ªë b·∫Øt bu·ªôc
    if not all([id_type, target_id, mode, from_date, to_date]):
        return jsonify({"error": "Thi·∫øu tham s·ªë b·∫Øt bu·ªôc"}), 400

    # Ki·ªÉm tra ƒë·ªãnh d·∫°ng ng√†y
    try:
        datetime.strptime(from_date, '%Y-%m-%d')
        datetime.strptime(to_date, '%Y-%m-%d')
    except ValueError:
        return jsonify({"error": "ƒê·ªãnh d·∫°ng ng√†y kh√¥ng h·ª£p l·ªá. S·ª≠ d·ª•ng YYYY-MM-DD"}), 400

    # Th·ª±c hi·ªán truy v·∫•n Spark
    user_count = processor.query_user_count(id_type, target_id, mode, from_date, to_date)

    # T√≠nh th·ªùi gian ph·∫£n h·ªìi
    end_time = datetime.now()
    query_time = (end_time - start_time).total_seconds()

    return jsonify({
        "success": True,
        "data": {
            "id_type": id_type,
            "id": target_id,
            "mode": mode,
            "from_date": from_date,
            "to_date": to_date,
            "user_count": user_count
        },
        "meta": {
            "query_time_seconds": round(query_time, 3),
            "timestamp": end_time.isoformat()
        }
    })

if __name__ == "__main__":
    if init_app():
        app.run(host="0.0.0.0", port=5000, debug=False, threaded=True)
```

#### **spark_session.py - C·∫•u H√¨nh Spark**
```python
from pyspark.sql import SparkSession
import os

def get_spark():
    """T·∫°o SparkSession v·ªõi c·∫•u h√¨nh t·ªëi ∆∞u"""
    mode = os.environ.get('SPARK_MODE', 'remote')

    if mode == 'remote':
        # K·∫øt n·ªëi v·ªõi Spark cluster
        spark = SparkSession.builder \
            .appName("AdnLogAPI-Remote") \
            .master("spark://adt-platform-dev-106-254:7077") \
            .config("spark.hadoop.fs.defaultFS", "hdfs://adt-platform-dev-106-254:8120") \
            .config("spark.sql.adaptive.enabled", "true") \
            .config("spark.sql.adaptive.coalescePartitions.enabled", "true") \
            .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") \
            .config("spark.sql.execution.arrow.pyspark.enabled", "true") \
            .config("spark.executor.memory", "2g") \
            .config("spark.executor.cores", "2") \
            .getOrCreate()
    elif mode == 'yarn':
        # S·∫£n xu·∫•t v·ªõi YARN
        spark = SparkSession.builder \
            .appName("AdnLogAPI-YARN") \
            .master("yarn") \
            .config("spark.hadoop.fs.defaultFS", "hdfs://adt-platform-dev-106-254:8120") \
            .config("spark.sql.adaptive.enabled", "true") \
            .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") \
            .getOrCreate()
    else:
        # Local mode cho ph√°t tri·ªÉn
        spark = SparkSession.builder \
            .appName("AdnLogAPI-Local") \
            .master("local[*]") \
            .config("spark.sql.adaptive.enabled", "true") \
            .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") \
            .getOrCreate()

    spark.sparkContext.setLogLevel("WARN")
    return spark
```

#### **data_processor.py - Logic X·ª≠ L√Ω D·ªØ Li·ªáu**
```python
from pyspark.sql.functions import from_unixtime, col, countDistinct, to_date
from pyspark.sql.types import StructType, StructField, StringType, BooleanType, LongType
import os

class AdnLogProcessor:
    def __init__(self, spark):
        self.spark = spark
        self.df = None

    def create_sample_data(self):
        """T·∫°o d·ªØ li·ªáu m·∫´u cho ph√°t tri·ªÉn"""
        schema = StructType([
            StructField("guid", StringType(), True),
            StructField("campaignId", StringType(), True),
            StructField("bannerId", StringType(), True),
            StructField("click_or_view", BooleanType(), True),
            StructField("time_create", LongType(), True)
        ])

        # D·ªØ li·ªáu m·∫´u v·ªõi c√°c test cases
        sample_data = [
            ("user1", "12345", "banner1", False, 1720137600000),  # 2024-07-05 view
            ("user2", "12345", "banner1", True, 1720137600000),   # 2024-07-05 click
            ("user3", "12345", "banner2", False, 1720051200000),  # 2024-07-04 view
            ("user1", "67890", "banner3", False, 1720051200000),  # 2024-07-04 view
            ("user4", "12345", "banner1", False, 1719964800000),  # 2024-07-03 view
            ("user5", "12345", "banner1", True, 1719964800000),   # 2024-07-03 click
        ]

        df = self.spark.createDataFrame(sample_data, schema)

        # Chuy·ªÉn ƒë·ªïi timestamp
        df_processed = df.withColumn(
            "event_time",
            from_unixtime(col("time_create") / 1000).cast("timestamp")
        ).withColumn(
            "event_date",
            to_date(col("event_time"))
        )

        return df_processed

    def load_and_cache_data(self):
        """T·∫£i v√† cache d·ªØ li·ªáu"""
        try:
            mode = os.environ.get('SPARK_MODE', 'remote')

            if mode in ['remote', 'yarn']:
                # S·∫£n xu·∫•t: ƒê·ªçc t·ª´ HDFS
                df = self.spark.read.parquet("hdfs://adt-platform-dev-106-254:8120/data/Parquet/AdnLog/*")
                df_processed = df.select(
                    "guid", "campaignId", "bannerId", "click_or_view",
                    col("time_group.time_create").alias("time_create")
                ).withColumn(
                    "event_time",
                    from_unixtime(col("time_create") / 1000).cast("timestamp")
                ).withColumn(
                    "event_date",
                    to_date(col("event_time"))
                )
            else:
                # Ph√°t tri·ªÉn: D·ªØ li·ªáu m·∫´u
                df_processed = self.create_sample_data()

            # Cache v√†o b·ªô nh·ªõ ƒë·ªÉ truy v·∫•n nhanh
            self.df = df_processed.cache()

            # K√≠ch ho·∫°t action ƒë·ªÉ cache th·ª±c s·ª±
            count = self.df.count()
            logger.info(f"Cached {count} records successfully")

            return True
        except Exception as e:
            logger.error(f"L·ªói t·∫£i d·ªØ li·ªáu: {str(e)}")
            return False

    def query_user_count(self, id_type, target_id, mode, from_date, to_date):
        """Truy v·∫•n s·ªë l∆∞·ª£ng user unique"""
        try:
            # Ki·ªÉm tra tham s·ªë
            if id_type not in ["campaignId", "bannerId"]:
                raise ValueError("id_type ph·∫£i l√† 'campaignId' ho·∫∑c 'bannerId'")

            if mode not in ["click", "view"]:
                raise ValueError("mode ph·∫£i l√† 'click' ho·∫∑c 'view'")

            # Chuy·ªÉn mode th√†nh boolean (click=true, view=false)
            is_click = (mode == "click")

            # X√¢y d·ª±ng chu·ªói truy v·∫•n hi·ªáu qu·∫£
            result = self.df.filter(col(id_type) == target_id) \
                           .filter(col("click_or_view") == is_click) \
                           .filter(col("event_date").between(from_date, to_date)) \
                           .agg(countDistinct("guid").alias("user_count")) \
                           .collect()

            return int(result[0]["user_count"]) if result else 0

        except Exception as e:
            logger.error(f"L·ªói truy v·∫•n: {str(e)}")
            raise
```

#### **wsgi.py - ƒêi·ªÉm Nh·∫≠p cho S·∫£n Xu·∫•t**
```python
# Giao di·ªán WSGI cho Gunicorn
# Kh·ªüi t·∫°o ·ª©ng d·ª•ng v·ªõi x·ª≠ l√Ω l·ªói
# C·∫•u h√¨nh logging cho s·∫£n xu·∫•t
```

---

### **2. Quy Tr√¨nh Tri·ªÉn Khai**

#### **B∆∞·ªõc 1: Chu·∫©n B·ªã M√¥i Tr∆∞·ªùng Server**

##### **1.1 Y√™u c·∫ßu h·ªá th·ªëng:**
- **H·ªá ƒëi·ªÅu h√†nh**: Linux (Ubuntu/CentOS)
- **Python**: 3.8+
- **Java**: 8+ (cho Spark)
- **Apache Spark**: 3.4.3+
- **B·ªô nh·ªõ**: 2GB+ RAM

##### **1.2 Ki·ªÉm tra m√¥i tr∆∞·ªùng:**
```bash
python3 --version  # Ph·∫£i >= 3.8
java -version      # Ph·∫£i >= 8
free -h           # Ki·ªÉm tra RAM
df -h             # Ki·ªÉm tra dung l∆∞·ª£ng ƒëƒ©a
```

##### **1.3 T·∫°o th∆∞ m·ª•c d·ª± √°n:**
```bash
mkdir -p ~/adnlog-api
cd ~/adnlog-api
pwd
```

##### **1.4 T·∫£i file qua Teleport:**
- Trong giao di·ªán web Teleport, t√¨m n√∫t "Files" ho·∫∑c "Upload".
- T·∫£i l√™n c√°c file:
  - `adnlog-api-complete.zip`

##### **1.5 Gi·∫£i n√©n file:**
```bash
cd ~/adnlog-api
unzip adnlog-api-complete.zip
```

##### **1.6 C·∫•p quy·ªÅn th·ª±c thi:**
```bash
chmod +x *.sh
```

---

#### **B∆∞·ªõc 2: C√†i ƒê·∫∑t M√¥i Tr∆∞·ªùng**

##### **2.1 C·∫•u h√¨nh bi·∫øn m√¥i tr∆∞·ªùng:**
```bash
export SPARK_HOME=/data/spark-3.4.3
export PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbin
export PYSPARK_PYTHON=python3
```

##### **2.2 Ch·∫°y Spark ·ªü Local Mode**
- T·∫°o file test:
  ```bash
  cat > spark_test_local.py << 'EOF'
  from pyspark.sql import SparkSession

  # T·∫°o Spark session v·ªõi local mode
  spark = SparkSession.builder \
      .appName("ADNLogTest") \
      .master("local[*]") \
      .config("spark.sql.warehouse.dir", "/tmp/spark-warehouse") \
      .getOrCreate()

  print("‚úÖ Spark session created!")
  print(f"Spark version: {spark.version}")
  print(f"Master: {spark.sparkContext.master}")

  df = spark.range(10)
  print(f"Test count: {df.count()}")

  spark.stop()
  print("‚úÖ Test completed!")
  EOF
  ```
- Ch·∫°y test:
  ```bash
  spark-submit --master local[*] spark_test_local.py
  ```

##### **2.3 C·∫•u h√¨nh PYTHONPATH:**
```bash
export PYTHONPATH=$SPARK_HOME/python:$SPARK_HOME/python/lib/py4j-0.10.9.7-src.zip:$PYTHONPATH
```
- Ki·ªÉm tra PySpark:
  ```bash
  python3 -c "from pyspark.sql import SparkSession; print('PySpark OK')"
  ```

---

#### **B∆∞·ªõc 3: Ch·∫°y ·ª®ng D·ª•ng**

##### **3.1 C√°ch 1: Ch·∫°y th·ªß c√¥ng v·ªõi thi·∫øt l·∫≠p m√¥i tr∆∞·ªùng**
```bash
cd ~/adnlog-api
export SPARK_HOME=/data/spark-3.4.3
export PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbin
export PYTHONPATH=$SPARK_HOME/python:$SPARK_HOME/python/lib/py4j-0.10.9.7-src.zip:$PYTHONPATH
export PYSPARK_PYTHON=python3
export SPARK_MODE=local
python3 app.py
```

##### **3.2 C√°ch 2: T·∫°o script t·ª± ƒë·ªông**
- T·∫°o `run_app.sh`:
  ```bash
  cat > run_app.sh << 'EOF'
  #!/bin/bash

  # Thi·∫øt l·∫≠p m√¥i tr∆∞·ªùng
  export SPARK_HOME=/data/spark-3.4.3
  export PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbin
  export PYTHONPATH=$SPARK_HOME/python:$SPARK_HOME/python/lib/py4j-0.10.9.7-src.zip:$PYTHONPATH
  export PYSPARK_PYTHON=python3
  export SPARK_MODE=local

  # Ch·∫°y ·ª©ng d·ª•ng
  python3 app.py
  EOF
  ```
- C·∫•p quy·ªÅn v√† ch·∫°y:
  ```bash
  chmod +x run_app.sh
  ./run_app.sh
  ```

##### **3.3 Ch·∫°y v·ªõi `spark-submit` (khuy·∫øn ngh·ªã):**
```bash
spark-submit --master local[*] app.py
```

---

#### **B∆∞·ªõc 4: Ch·∫°y Server v·ªõi Gunicorn**

##### **4.1 T·∫°o script kh·ªüi ƒë·ªông server**
```bash
cd ~/adnlog-api
cat > start_server.sh << 'EOF'
#!/bin/bash
echo "üöÄ Kh·ªüi ƒë·ªông Server AdnLog API..."

# Thi·∫øt l·∫≠p m√¥i tr∆∞·ªùng
export SPARK_HOME=/data/spark-3.4.3
export PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbin
export PYTHONPATH=$SPARK_HOME/python:$SPARK_HOME/python/lib/py4j-0.10.9.7-src.zip:$PYTHONPATH
export PYSPARK_PYTHON=python3
export SPARK_MODE=local

echo "üìç Server s·∫Ω s·∫µn s√†ng t·∫°i: http://localhost:5000"
echo "üìñ T√†i li·ªáu API: http://localhost:5000/"
echo "‚ù§Ô∏è Ki·ªÉm tra s·ª©c kh·ªèe: http://localhost:5000/health"
echo ""

# Kh·ªüi ƒë·ªông server
gunicorn --bind 0.0.0.0:5000 --workers 2 --timeout 120 wsgi:app
EOF
```

##### **4.2 T·∫°o script ki·ªÉm tra API**
```bash
cat > test_api.sh << 'EOF'
#!/bin/bash
echo "üß™ Ki·ªÉm tra AdnLog API..."

echo "1. Ki·ªÉm tra s·ª©c kh·ªèe:"
curl -s http://localhost:5000/health | python3 -m json.tool

echo -e "\n2. T√†i li·ªáu API:"
curl -s http://localhost:5000/ | python3 -m json.tool

echo -e "\n3. Ki·ªÉm tra truy v·∫•n Campaign View:"
curl -s "http://localhost:5000/query?id_type=campaignId&id=12345&mode=view&from=2024-07-03&to=2024-07-05" | python3 -m json.tool

echo -e "\n4. Ki·ªÉm tra truy v·∫•n Campaign Click:"
curl -s "http://localhost:5000/query?id_type=campaignId&id=12345&mode=click&from=2024-07-03&to=2024-07-05" | python3 -m json.tool

echo -e "\n‚úÖ Ho√†n t·∫•t t·∫•t c·∫£ c√°c ki·ªÉm tra!"
EOF
```

##### **4.3 C·∫•p quy·ªÅn v√† ch·∫°y**
```bash
chmod +x start_server.sh test_api.sh
echo "‚úÖ Scripts ƒë√£ ƒë∆∞·ª£c t·∫°o th√†nh c√¥ng!"
./start_server.sh
```

##### **4.4 Thay ƒë·ªïi c·ªïng (t√πy ch·ªçn)**
- Thay ƒë·ªïi c·ªïng trong `app.py`:
  ```bash
  sed -i 's/port=5000/port=8080/' app.py
  python3 app.py
  ```
- Ho·∫∑c d√πng l·ªánh d·ª± ph√≤ng:
  ```bash
  gunicorn --bind 0.0.0.0:5000 --workers 2 --timeout 120 wsgi:app
  ```

##### **4.5 Ki·ªÉm tra API**
- M·ªü terminal m·ªõi v√† ch·∫°y:
  ```bash
  # Ki·ªÉm tra s·ª©c kh·ªèe
  curl http://localhost:5000/health

  # V√≠ d·ª• truy v·∫•n
  curl "http://localhost:5000/query?id_type=campaignId&id=12345&mode=view&from=2024-07-03&to=2024-07-05"
  ```

---

#### **B∆∞·ªõc 5: Tri·ªÉn Khai T·ª± ƒê·ªông**
```bash
# T·∫£i adnlog-api-complete.zip l√™n server
unzip adnlog-api-complete.zip
cd adnlog-api-complete/
chmod +x *.sh
./setup_environment.sh
./start_server.sh
```

---

## ‚úÖ X√°c Th·ª±c Ch·ª©c NƒÉng
1. **Ch·ª©c nƒÉng c·ªët l√µi**:
   - ‚úì Tr·∫£ v·ªÅ s·ªë user view/click: `"user_count": 3`
   - ‚úì H·ªó tr·ª£ campaign: `"id_type": "campaignId", "id": "12345"`
   - ‚úì H·ªó tr·ª£ banner: API c√≥ endpoint cho `bannerId`
   - ‚úì L·ªçc theo th·ªùi gian: `"from_date": "2024-07-03", "to_date": "2024-07-05"`
   - ‚úì Ph√¢n bi·ªát view/click: `"mode": "view"` (false = view, true = click)

2. **Hi·ªáu su·∫•t**:
   - Y√™u c·∫ßu: < 1 ph√∫t
   - Th·ª±c t·∫ø: `"query_time_seconds": 0.929` (< 1 gi√¢y!)
   - K·∫øt qu·∫£: Nhanh h∆°n y√™u c·∫ßu 60 l·∫ßn! üöÄ

3. **C·∫•u tr√∫c d·ªØ li·ªáu**:
   - Server hi·ªÉu ƒë√∫ng c·∫•u tr√∫c log:
     - `guid` ‚Üí ƒê·ªãnh danh user ‚úì
     - `campaignId` ‚Üí ID chi·∫øn d·ªãch ‚úì
     - `bannerId` ‚Üí ID banner ‚úì
     - `click_or_view` ‚Üí false=view, true=click ‚úì

4. **Thi·∫øt k·∫ø API**:
   - RESTful: Endpoint GET v·ªõi tham s·ªë truy v·∫•n
   - Linh ho·∫°t: H·ªó tr·ª£ c·∫£ `campaignId` v√† `bannerId`
   - R√µ r√†ng: ƒê·ªãnh d·∫°ng ph·∫£n h·ªìi r√µ r√†ng v·ªõi metadata
   - X·ª≠ l√Ω l·ªói: Ki·ªÉm tra tham s·ªë ƒë·∫ßu v√†o

#### **Tr∆∞·ªùng h·ª£p ki·ªÉm tra**
```bash
# Ki·ªÉm tra truy v·∫•n banner
curl "http://localhost:5000/query?id_type=bannerId&id=banner1&mode=click&from=2024-07-03&to=2024-07-05"

# Ki·ªÉm tra tr∆∞·ªùng h·ª£p bi√™n
curl "http://localhost:5000/query?id_type=campaignId&id=99999&mode=view&from=2024-07-01&to=2024-07-02"
```

---
