# ğŸš€ HÆ°á»›ng Dáº«n Triá»ƒn Khai vÃ  Triá»ƒn Khai Dá»± Ãn AdnLog API

## ğŸ“… NgÃ y vÃ  Giá» Hiá»‡n Táº¡i
- **NgÃ y**: Chá»§ Nháº­t, 27 thÃ¡ng 7 nÄƒm 2025
- **Thá»i gian**: 19:41 PM +07

## ğŸ“‹ Tá»•ng Quan BÃ i ToÃ¡n
**YÃªu cáº§u**: XÃ¢y dá»±ng server API tráº£ vá» sá»‘ lÆ°á»£ng user view/click cho campaign/banner theo khoáº£ng thá»i gian, vá»›i thá»i gian pháº£n há»“i < 1 phÃºt.

**Äáº§u vÃ o**: 
- Log quáº£ng cÃ¡o vá»›i cÃ¡c trÆ°á»ng: `guid` (ID ngÆ°á»i dÃ¹ng), `campaignId`, `bannerId`, `click_or_view` (false=view, true=click), `time_create`.

**Äáº§u ra**: 
- Endpoint API tráº£ vá» sá»‘ lÆ°á»£ng user unique Ä‘Ã£ view/click campaign/banner trong khoáº£ng thá»i gian Ä‘Ã£ cho.

---

## ğŸ—ï¸ Kiáº¿n TrÃºc Giáº£i PhÃ¡p
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Flask API     â”‚â”€â”€â”€â–¶â”‚  Apache Spark   â”‚â”€â”€â–¶â”‚ Nguá»“n Dá»¯ Liá»‡u   â”‚
â”‚   (REST API)    â”‚    â”‚  (Xá»­ lÃ½)        â”‚    â”‚ (HDFS/Máº«u)      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚                       â”‚                       â”‚
        â”‚                       â”‚                       â”‚
    Gunicorn              Bá»™ Nhá»› Cache            File Parquet
   (Sáº£n xuáº¥t)           (Hiá»‡u suáº¥t)            (LÆ°u trá»¯)
```

**Chiáº¿n lÆ°á»£c chÃ­nh**: Cache toÃ n bá»™ dá»¯ liá»‡u vÃ o bá»™ nhá»› â†’ Thá»±c hiá»‡n truy váº¥n tá»« bá»™ nhá»› thay vÃ¬ Ä‘Ä©a â†’ Thá»i gian pháº£n há»“i < 1 giÃ¢y.

---

## ğŸ’» Chi Tiáº¿t Triá»ƒn Khai

### **1. CÃ¡c File Cá»‘t LÃµi cá»§a á»¨ng Dá»¥ng**

#### **app.py - á»¨ng dá»¥ng Flask ChÃ­nh**
```python
from flask import Flask, request, jsonify
from datetime import datetime
from spark_session import get_spark
from data_processor import AdnLogProcessor

app = Flask(__name__)
spark = None
processor = None

def init_app():
    """Khá»Ÿi táº¡o Spark session vÃ  táº£i dá»¯ liá»‡u"""
    global spark, processor
    try:
        # BÆ°á»›c 1: Khá»Ÿi táº¡o Spark session
        spark = get_spark()

        # BÆ°á»›c 2: Khá»Ÿi táº¡o bá»™ xá»­ lÃ½ dá»¯ liá»‡u
        processor = AdnLogProcessor(spark)

        # BÆ°á»›c 3: Táº£i vÃ  cache dá»¯ liá»‡u
        if processor.load_and_cache_data():
            return True
        return False
    except Exception as e:
        logger.error(f"Lá»—i khá»Ÿi táº¡o: {str(e)}")
        return False

@app.route("/query", methods=["GET"])
def query_user_count():
    """Endpoint chÃ­nh Ä‘á»ƒ truy váº¥n"""
    start_time = datetime.now()

    # Láº¥y tham sá»‘
    id_type = request.args.get("id_type")
    target_id = request.args.get("id")
    mode = request.args.get("mode")
    from_date = request.args.get("from")
    to_date = request.args.get("to")

    # Kiá»ƒm tra tham sá»‘ báº¯t buá»™c
    if not all([id_type, target_id, mode, from_date, to_date]):
        return jsonify({"error": "Thiáº¿u tham sá»‘ báº¯t buá»™c"}), 400

    # Kiá»ƒm tra Ä‘á»‹nh dáº¡ng ngÃ y
    try:
        datetime.strptime(from_date, '%Y-%m-%d')
        datetime.strptime(to_date, '%Y-%m-%d')
    except ValueError:
        return jsonify({"error": "Äá»‹nh dáº¡ng ngÃ y khÃ´ng há»£p lá»‡. Sá»­ dá»¥ng YYYY-MM-DD"}), 400

    # Thá»±c hiá»‡n truy váº¥n Spark
    user_count = processor.query_user_count(id_type, target_id, mode, from_date, to_date)

    # TÃ­nh thá»i gian pháº£n há»“i
    end_time = datetime.now()
    query_time = (end_time - start_time).total_seconds()

    return jsonify({
        "success": True,
        "data": {
            "id_type": id_type,
            "id": target_id,
            "mode": mode,
            "from_date": from_date,
            "to_date": to_date,
            "user_count": user_count
        },
        "meta": {
            "query_time_seconds": round(query_time, 3),
            "timestamp": end_time.isoformat()
        }
    })

if __name__ == "__main__":
    if init_app():
        app.run(host="0.0.0.0", port=5000, debug=False, threaded=True)
```

#### **spark_session.py - Cáº¥u HÃ¬nh Spark**
```python
from pyspark.sql import SparkSession
import os

def get_spark():
    """Táº¡o SparkSession vá»›i cáº¥u hÃ¬nh tá»‘i Æ°u"""
    mode = os.environ.get('SPARK_MODE', 'remote')

    if mode == 'remote':
        # Káº¿t ná»‘i vá»›i Spark cluster
        spark = SparkSession.builder \
            .appName("AdnLogAPI-Remote") \
            .master("spark://adt-platform-dev-106-254:7077") \
            .config("spark.hadoop.fs.defaultFS", "hdfs://adt-platform-dev-106-254:8120") \
            .config("spark.sql.adaptive.enabled", "true") \
            .config("spark.sql.adaptive.coalescePartitions.enabled", "true") \
            .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") \
            .config("spark.sql.execution.arrow.pyspark.enabled", "true") \
            .config("spark.executor.memory", "2g") \
            .config("spark.executor.cores", "2") \
            .getOrCreate()
    elif mode == 'yarn':
        # Sáº£n xuáº¥t vá»›i YARN
        spark = SparkSession.builder \
            .appName("AdnLogAPI-YARN") \
            .master("yarn") \
            .config("spark.hadoop.fs.defaultFS", "hdfs://adt-platform-dev-106-254:8120") \
            .config("spark.sql.adaptive.enabled", "true") \
            .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") \
            .getOrCreate()
    else:
        # Local mode cho phÃ¡t triá»ƒn
        spark = SparkSession.builder \
            .appName("AdnLogAPI-Local") \
            .master("local[*]") \
            .config("spark.sql.adaptive.enabled", "true") \
            .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") \
            .getOrCreate()

    spark.sparkContext.setLogLevel("WARN")
    return spark
```

#### **data_processor.py - Logic Xá»­ LÃ½ Dá»¯ Liá»‡u**
```python
from pyspark.sql.functions import from_unixtime, col, countDistinct, to_date
from pyspark.sql.types import StructType, StructField, StringType, BooleanType, LongType
import os

class AdnLogProcessor:
    def __init__(self, spark):
        self.spark = spark
        self.df = None

    def create_sample_data(self):
        """Táº¡o dá»¯ liá»‡u máº«u cho phÃ¡t triá»ƒn"""
        schema = StructType([
            StructField("guid", StringType(), True),
            StructField("campaignId", StringType(), True),
            StructField("bannerId", StringType(), True),
            StructField("click_or_view", BooleanType(), True),
            StructField("time_create", LongType(), True)
        ])

        # Dá»¯ liá»‡u máº«u vá»›i cÃ¡c test cases
        sample_data = [
            ("user1", "12345", "banner1", False, 1720137600000),  # 2024-07-05 view
            ("user2", "12345", "banner1", True, 1720137600000),   # 2024-07-05 click
            ("user3", "12345", "banner2", False, 1720051200000),  # 2024-07-04 view
            ("user1", "67890", "banner3", False, 1720051200000),  # 2024-07-04 view
            ("user4", "12345", "banner1", False, 1719964800000),  # 2024-07-03 view
            ("user5", "12345", "banner1", True, 1719964800000),   # 2024-07-03 click
        ]

        df = self.spark.createDataFrame(sample_data, schema)

        # Chuyá»ƒn Ä‘á»•i timestamp
        df_processed = df.withColumn(
            "event_time",
            from_unixtime(col("time_create") / 1000).cast("timestamp")
        ).withColumn(
            "event_date",
            to_date(col("event_time"))
        )

        return df_processed

    def load_and_cache_data(self):
        """Táº£i vÃ  cache dá»¯ liá»‡u"""
        try:
            mode = os.environ.get('SPARK_MODE', 'remote')

            if mode in ['remote', 'yarn']:
                # Sáº£n xuáº¥t: Äá»c tá»« HDFS
                df = self.spark.read.parquet("hdfs://adt-platform-dev-106-254:8120/data/Parquet/AdnLog/*")
                df_processed = df.select(
                    "guid", "campaignId", "bannerId", "click_or_view",
                    col("time_group.time_create").alias("time_create")
                ).withColumn(
                    "event_time",
                    from_unixtime(col("time_create") / 1000).cast("timestamp")
                ).withColumn(
                    "event_date",
                    to_date(col("event_time"))
                )
            else:
                # PhÃ¡t triá»ƒn: Dá»¯ liá»‡u máº«u
                df_processed = self.create_sample_data()

            # Cache vÃ o bá»™ nhá»› Ä‘á»ƒ truy váº¥n nhanh
            self.df = df_processed.cache()

            # KÃ­ch hoáº¡t action Ä‘á»ƒ cache thá»±c sá»±
            count = self.df.count()
            logger.info(f"Cached {count} records successfully")

            return True
        except Exception as e:
            logger.error(f"Lá»—i táº£i dá»¯ liá»‡u: {str(e)}")
            return False

    def query_user_count(self, id_type, target_id, mode, from_date, to_date):
        """Truy váº¥n sá»‘ lÆ°á»£ng user unique"""
        try:
            # Kiá»ƒm tra tham sá»‘
            if id_type not in ["campaignId", "bannerId"]:
                raise ValueError("id_type pháº£i lÃ  'campaignId' hoáº·c 'bannerId'")

            if mode not in ["click", "view"]:
                raise ValueError("mode pháº£i lÃ  'click' hoáº·c 'view'")

            # Chuyá»ƒn mode thÃ nh boolean (click=true, view=false)
            is_click = (mode == "click")

            # XÃ¢y dá»±ng chuá»—i truy váº¥n hiá»‡u quáº£
            result = self.df.filter(col(id_type) == target_id) \
                           .filter(col("click_or_view") == is_click) \
                           .filter(col("event_date").between(from_date, to_date)) \
                           .agg(countDistinct("guid").alias("user_count")) \
                           .collect()

            return int(result[0]["user_count"]) if result else 0

        except Exception as e:
            logger.error(f"Lá»—i truy váº¥n: {str(e)}")
            raise
```

#### **wsgi.py - Äiá»ƒm Nháº­p cho Sáº£n Xuáº¥t**
```python
# Giao diá»‡n WSGI cho Gunicorn
# Khá»Ÿi táº¡o á»©ng dá»¥ng vá»›i xá»­ lÃ½ lá»—i
# Cáº¥u hÃ¬nh logging cho sáº£n xuáº¥t
```

---

### **2. Quy TrÃ¬nh Triá»ƒn Khai**

#### **BÆ°á»›c 1: Chuáº©n Bá»‹ MÃ´i TrÆ°á»ng Server**

##### **1.1 YÃªu cáº§u há»‡ thá»‘ng:**
- **Há»‡ Ä‘iá»u hÃ nh**: Linux (Ubuntu/CentOS)
- **Python**: 3.8+
- **Java**: 8+ (cho Spark)
- **Apache Spark**: 3.4.3+
- **Bá»™ nhá»›**: 2GB+ RAM

##### **1.2 Kiá»ƒm tra mÃ´i trÆ°á»ng:**
```bash
python3 --version  # Pháº£i >= 3.8
java -version      # Pháº£i >= 8
free -h           # Kiá»ƒm tra RAM
df -h             # Kiá»ƒm tra dung lÆ°á»£ng Ä‘Ä©a
```

##### **1.3 Táº¡o thÆ° má»¥c dá»± Ã¡n:**
```bash
mkdir -p ~/adnlog-api
cd ~/adnlog-api
pwd
```

##### **1.4 Táº£i file qua Teleport:**
- Trong giao diá»‡n web Teleport, tÃ¬m nÃºt "Files" hoáº·c "Upload".
- Táº£i lÃªn cÃ¡c file:
  - `adnlog-api-complete.zip`

##### **1.5 Giáº£i nÃ©n file:**
```bash
cd ~/adnlog-api
unzip adnlog-api-complete.zip
```

##### **1.6 Cáº¥p quyá»n thá»±c thi:**
```bash
chmod +x *.sh
```

---

#### **BÆ°á»›c 2: CÃ i Äáº·t MÃ´i TrÆ°á»ng**

##### **2.1 Cáº¥u hÃ¬nh biáº¿n mÃ´i trÆ°á»ng:**
```bash
export SPARK_HOME=/data/spark-3.4.3
export PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbin
export PYSPARK_PYTHON=python3
```

##### **2.2 Cháº¡y Spark á»Ÿ Local Mode**
- Táº¡o file test:
  ```bash
  cat > spark_test_local.py << 'EOF'
  from pyspark.sql import SparkSession

  # Táº¡o Spark session vá»›i local mode
  spark = SparkSession.builder \
      .appName("ADNLogTest") \
      .master("local[*]") \
      .config("spark.sql.warehouse.dir", "/tmp/spark-warehouse") \
      .getOrCreate()

  print("âœ… Spark session created!")
  print(f"Spark version: {spark.version}")
  print(f"Master: {spark.sparkContext.master}")

  df = spark.range(10)
  print(f"Test count: {df.count()}")

  spark.stop()
  print("âœ… Test completed!")
  EOF
  ```
- Cháº¡y test:
  ```bash
  spark-submit --master local[*] spark_test_local.py
  ```

##### **2.3 Cáº¥u hÃ¬nh PYTHONPATH:**
```bash
export PYTHONPATH=$SPARK_HOME/python:$SPARK_HOME/python/lib/py4j-0.10.9.7-src.zip:$PYTHONPATH
```
- Kiá»ƒm tra PySpark:
  ```bash
  python3 -c "from pyspark.sql import SparkSession; print('PySpark OK')"
  ```

---

#### **BÆ°á»›c 3: Cháº¡y á»¨ng Dá»¥ng**

##### **3.1 CÃ¡ch 1: Cháº¡y thá»§ cÃ´ng vá»›i thiáº¿t láº­p mÃ´i trÆ°á»ng**
```bash
cd ~/adnlog-api
export SPARK_HOME=/data/spark-3.4.3
export PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbin
export PYTHONPATH=$SPARK_HOME/python:$SPARK_HOME/python/lib/py4j-0.10.9.7-src.zip:$PYTHONPATH
export PYSPARK_PYTHON=python3
export SPARK_MODE=local
python3 app.py
```

##### **3.2 CÃ¡ch 2: Táº¡o script tá»± Ä‘á»™ng**
- Táº¡o `run_app.sh`:
  ```bash
  cat > run_app.sh << 'EOF'
  #!/bin/bash

  # Thiáº¿t láº­p mÃ´i trÆ°á»ng
  export SPARK_HOME=/data/spark-3.4.3
  export PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbin
  export PYTHONPATH=$SPARK_HOME/python:$SPARK_HOME/python/lib/py4j-0.10.9.7-src.zip:$PYTHONPATH
  export PYSPARK_PYTHON=python3
  export SPARK_MODE=local

  # Cháº¡y á»©ng dá»¥ng
  python3 app.py
  EOF
  ```
- Cáº¥p quyá»n vÃ  cháº¡y:
  ```bash
  chmod +x run_app.sh
  ./run_app.sh
  ```

##### **3.3 Cháº¡y vá»›i `spark-submit` (khuyáº¿n nghá»‹):**
```bash
spark-submit --master local[*] app.py
```

---

#### **BÆ°á»›c 4: Cháº¡y Server vá»›i Gunicorn**

##### **4.1 Táº¡o script khá»Ÿi Ä‘á»™ng server**
```bash
cd ~/adnlog-api
cat > start_server.sh << 'EOF'
#!/bin/bash
echo "ğŸš€ Khá»Ÿi Ä‘á»™ng Server AdnLog API..."

# Thiáº¿t láº­p mÃ´i trÆ°á»ng
export SPARK_HOME=/data/spark-3.4.3
export PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbin
export PYTHONPATH=$SPARK_HOME/python:$SPARK_HOME/python/lib/py4j-0.10.9.7-src.zip:$PYTHONPATH
export PYSPARK_PYTHON=python3
export SPARK_MODE=local

echo "ğŸ“ Server sáº½ sáºµn sÃ ng táº¡i: http://localhost:5000"
echo "ğŸ“– TÃ i liá»‡u API: http://localhost:5000/"
echo "â¤ï¸ Kiá»ƒm tra sá»©c khá»e: http://localhost:5000/health"
echo ""

# Khá»Ÿi Ä‘á»™ng server
gunicorn --bind 0.0.0.0:5000 --workers 2 --timeout 120 wsgi:app
EOF
```

##### **4.2 Táº¡o script kiá»ƒm tra API**
```bash
cat > test_api.sh << 'EOF'
#!/bin/bash
echo "ğŸ§ª Kiá»ƒm tra AdnLog API..."

echo "1. Kiá»ƒm tra sá»©c khá»e:"
curl -s http://localhost:5000/health | python3 -m json.tool

echo -e "\n2. TÃ i liá»‡u API:"
curl -s http://localhost:5000/ | python3 -m json.tool

echo -e "\n3. Kiá»ƒm tra truy váº¥n Campaign View:"
curl -s "http://localhost:5000/query?id_type=campaignId&id=12345&mode=view&from=2024-07-03&to=2024-07-05" | python3 -m json.tool

echo -e "\n4. Kiá»ƒm tra truy váº¥n Campaign Click:"
curl -s "http://localhost:5000/query?id_type=campaignId&id=12345&mode=click&from=2024-07-03&to=2024-07-05" | python3 -m json.tool

echo -e "\nâœ… HoÃ n táº¥t táº¥t cáº£ cÃ¡c kiá»ƒm tra!"
EOF
```

##### **4.3 Cáº¥p quyá»n vÃ  cháº¡y**
```bash
chmod +x start_server.sh test_api.sh
echo "âœ… Scripts Ä‘Ã£ Ä‘Æ°á»£c táº¡o thÃ nh cÃ´ng!"
./start_server.sh
```

##### **4.4 Thay Ä‘á»•i cá»•ng (tÃ¹y chá»n)**
- Thay Ä‘á»•i cá»•ng trong `app.py`:
  ```bash
  sed -i 's/port=5000/port=8080/' app.py
  python3 app.py
  ```
- Hoáº·c dÃ¹ng lá»‡nh dá»± phÃ²ng:
  ```bash
  gunicorn --bind 0.0.0.0:5000 --workers 2 --timeout 120 wsgi:app
  ```

##### **4.5 Kiá»ƒm tra API**
- Má»Ÿ terminal má»›i vÃ  cháº¡y:
  ```bash
  # Kiá»ƒm tra sá»©c khá»e
  curl http://localhost:5000/health

  # VÃ­ dá»¥ truy váº¥n
  curl "http://localhost:5000/query?id_type=campaignId&id=12345&mode=view&from=2024-07-03&to=2024-07-05"
  ```

---

#### **BÆ°á»›c 5: Triá»ƒn Khai Tá»± Äá»™ng**
```bash
# Táº£i adnlog-api-complete.zip lÃªn server
unzip adnlog-api-complete.zip
cd adnlog-api-complete/
chmod +x *.sh
./setup_environment.sh
./start_server.sh
```

---

## âœ… XÃ¡c Thá»±c Chá»©c NÄƒng
1. **Chá»©c nÄƒng cá»‘t lÃµi**:
   - âœ“ Tráº£ vá» sá»‘ user view/click: `"user_count": 3`
   - âœ“ Há»— trá»£ campaign: `"id_type": "campaignId", "id": "12345"`
   - âœ“ Há»— trá»£ banner: API cÃ³ endpoint cho `bannerId`
   - âœ“ Lá»c theo thá»i gian: `"from_date": "2024-07-03", "to_date": "2024-07-05"`
   - âœ“ PhÃ¢n biá»‡t view/click: `"mode": "view"` (false = view, true = click)

2. **Hiá»‡u suáº¥t**:
   - YÃªu cáº§u: < 1 phÃºt
   - Thá»±c táº¿: `"query_time_seconds": 0.929` (< 1 giÃ¢y!)
   - Káº¿t quáº£: Nhanh hÆ¡n yÃªu cáº§u 60 láº§n! ğŸš€

3. **Cáº¥u trÃºc dá»¯ liá»‡u**:
   - Server hiá»ƒu Ä‘Ãºng cáº¥u trÃºc log:
     - `guid` â†’ Äá»‹nh danh user âœ“
     - `campaignId` â†’ ID chiáº¿n dá»‹ch âœ“
     - `bannerId` â†’ ID banner âœ“
     - `click_or_view` â†’ false=view, true=click âœ“

4. **Thiáº¿t káº¿ API**:
   - RESTful: Endpoint GET vá»›i tham sá»‘ truy váº¥n
   - Linh hoáº¡t: Há»— trá»£ cáº£ `campaignId` vÃ  `bannerId`
   - RÃµ rÃ ng: Äá»‹nh dáº¡ng pháº£n há»“i rÃµ rÃ ng vá»›i metadata
   - Xá»­ lÃ½ lá»—i: Kiá»ƒm tra tham sá»‘ Ä‘áº§u vÃ o

#### **TrÆ°á»ng há»£p kiá»ƒm tra**
```bash
# Kiá»ƒm tra truy váº¥n banner
curl "http://localhost:5000/query?id_type=bannerId&id=banner1&mode=click&from=2024-07-03&to=2024-07-05"

# Kiá»ƒm tra trÆ°á»ng há»£p biÃªn
curl "http://localhost:5000/query?id_type=campaignId&id=99999&mode=view&from=2024-07-01&to=2024-07-02"
```

---

## ğŸ”§ Gá»¡ Lá»—i vÃ  TÃ­ch Há»£p HDFS

### **1. Chuyá»ƒn sang Remote/YARN Mode**
#### **TÃ¹y chá»n A: Remote Mode**
```bash
export SPARK_MODE=remote
export SPARK_HOME=/data/spark-3.4.3
export PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbin
export PYTHONPATH=$SPARK_HOME/python:$SPARK_HOME/python/lib/py4j-0.10.9.7-src.zip:$PYTHONPATH
export PYSPARK_PYTHON=python3
```

### **2. Kiá»ƒm tra Dá»¯ liá»‡u HDFS**
```bash
# Kiá»ƒm tra kháº£ nÄƒng truy cáº­p HDFS
hdfs dfs -ls /data/Parquet/AdnLog/

# Xem cáº¥u trÃºc thÆ° má»¥c
hdfs dfs -ls -R /data/Parquet/AdnLog/ | head -20

# Kiá»ƒm tra dung lÆ°á»£ng file
hdfs dfs -du -h /data/Parquet/AdnLog/

# Kiá»ƒm tra Ä‘á»c file Parquet
hdfs dfs -cat /data/Parquet/AdnLog/part-00000.parquet | head -10
```

### **3. Kiá»ƒm tra Cáº¥u hÃ¬nh Hadoop**
```bash
# Kiá»ƒm tra file cáº¥u hÃ¬nh Hadoop
ls -la $HADOOP_HOME/etc/hadoop/
cat $HADOOP_HOME/etc/hadoop/core-site.xml
cat $HADOOP_HOME/etc/hadoop/hdfs-site.xml

# Hoáº·c tÃ¬m file cáº¥u hÃ¬nh
find /etc -name "core-site.xml" 2>/dev/null
find /opt -name "core-site.xml" 2>/dev/null
find /data -name "core-site.xml" 2>/dev/null
```

### **4. Sá»­a Cáº¥u hÃ¬nh HDFS**
```bash
# Thiáº¿t láº­p cáº¥u hÃ¬nh Hadoop qua biáº¿n mÃ´i trÆ°á»ng
export HADOOP_CONF_DIR=/data/hadoop-3.3.5/etc/hadoop
export HDFS_NAMENODE_USER=hdfs1
export HDFS_DATANODE_USER=hdfs1
export HADOOP_OPTS="-Dfs.defaultFS=hdfs://adt-platform-dev-106-254:8120"
```

### **5. Kiá»ƒm tra Spark vá»›i HDFS**
- Táº¡o script test:
  ```bash
  cat > test_spark_hdfs.py << 'EOF'
  #!/usr/bin/env python3
  import os
  from pyspark.sql import SparkSession

  try:
      # Táº¡o Spark session vá»›i cáº¥u hÃ¬nh HDFS override
      spark = SparkSession.builder \
          .appName("HDFS-Direct-Test") \
          .master("local[*]") \
          .config("spark.hadoop.fs.defaultFS", "hdfs://adt-platform-dev-106-254:8120") \
          .config("spark.hadoop.dfs.nameservices", "") \
          .config("spark.hadoop.dfs.client.failover.proxy.provider", "") \
          .getOrCreate()
      
      print("âœ… Spark session created!")
      
      # Test cÃ¡c Ä‘Æ°á»ng dáº«n HDFS
      hdfs_paths = [
          "hdfs://adt-platform-dev-106-254:8120/data/Parquet/AdnLog/*",
          "hdfs://10.3.106.254:8120/data/Parquet/AdnLog/*",
          "/data/Parquet/AdnLog/*"  # ÄÆ°á»ng dáº«n cá»¥c bá»™ náº¿u dá»¯ liá»‡u Ä‘Æ°á»£c gáº¯n
      ]
      
      for path in hdfs_paths:
          try:
              print(f"\nğŸ” Kiá»ƒm tra Ä‘Æ°á»ng dáº«n: {path}")
              df = spark.read.parquet(path)
              count = df.count()
              print(f"âœ… ThÃ nh cÃ´ng! TÃ¬m tháº¥y {count} báº£n ghi")
              
              # Hiá»ƒn thá»‹ schema
              print("ğŸ“‹ Schema:")
              df.printSchema()
              
              # Hiá»ƒn thá»‹ máº«u dá»¯ liá»‡u
              print("ğŸ“„ Dá»¯ liá»‡u máº«u:")
              df.show(3)
              break
              
          except Exception as e:
              print(f"âŒ Tháº¥t báº¡i: {str(e)}")
              continue
      
      spark.stop()
      
  except Exception as e:
      print(f"âŒ Lá»—i Spark session: {str(e)}")
  EOF
  ```
- Cháº¡y test:
  ```bash
  python3 test_spark_hdfs.py
  ```

---

### **6. Chuyá»ƒn sang Dá»¯ liá»‡u HDFS**

#### **6.1 Táº¡o á»¨ng Dá»¥ng Hybrid**
- Táº¡o `app_hybrid.py`:
  ```bash
  cat > app_hybrid.py << 'EOF'
  from flask import Flask, request, jsonify
  from datetime import datetime
  from spark_session import get_spark
  from data_processor import AdnLogProcessor
  import logging

  # Cáº¥u hÃ¬nh logging
  logging.basicConfig(level=logging.INFO)
  logger = logging.getLogger(__name__)

  app = Flask(__name__)
  spark = None
  processor = None

  def init_app():
      """Khá»Ÿi táº¡o Spark session vÃ  táº£i dá»¯ liá»‡u"""
      global spark, processor
      try:
          logger.info("=== Báº¯t Ä‘áº§u khá»Ÿi táº¡o Server AdnLog ===")
          logger.info("BÆ°á»›c 1: Khá»Ÿi táº¡o Spark session...")
          
          spark = get_spark()
          logger.info("âœ“ Spark session Ä‘Æ°á»£c táº¡o thÃ nh cÃ´ng")
          
          logger.info("BÆ°á»›c 2: Khá»Ÿi táº¡o bá»™ xá»­ lÃ½ dá»¯ liá»‡u...")
          processor = AdnLogProcessor(spark)
          logger.info("âœ“ Bá»™ xá»­ lÃ½ dá»¯ liá»‡u Ä‘Æ°á»£c khá»Ÿi táº¡o")
          
          logger.info("BÆ°á»›c 3: Táº£i vÃ  cache dá»¯ liá»‡u...")
          if processor.load_and_cache_data():
              logger.info("âœ“ Dá»¯ liá»‡u Ä‘Æ°á»£c táº£i vÃ  cache thÃ nh cÃ´ng")
              logger.info("=== HoÃ n táº¥t khá»Ÿi táº¡o server ===")
              return True
          else:
              logger.error("âœ— Tháº¥t báº¡i khi táº£i dá»¯ liá»‡u")
              return False
              
      except Exception as e:
          logger.error(f"Lá»—i khá»Ÿi táº¡o: {str(e)}")
          return False

  @app.route("/", methods=["GET"])
  def api_documentation():
      """Endpoint tÃ i liá»‡u API"""
      return jsonify({
          "service": "AdnLog Query API",
          "version": "1.0.0",
          "endpoints": {
              "/health": "Kiá»ƒm tra sá»©c khá»e",
              "/query": "Truy váº¥n sá»‘ lÆ°á»£ng user cho campaign/banner"
          },
          "query_parameters": {
              "id_type": "campaignId hoáº·c bannerId",
              "id": "GiÃ¡ trá»‹ ID má»¥c tiÃªu", 
              "mode": "click hoáº·c view",
              "from": "NgÃ y báº¯t Ä‘áº§u (YYYY-MM-DD)",
              "to": "NgÃ y káº¿t thÃºc (YYYY-MM-DD)"
          },
          "example": "/query?id_type=campaignId&id=12345&mode=view&from=2024-07-01&to=2024-07-05"
      })

  @app.route("/health", methods=["GET"])
  def health_check():
      """Endpoint kiá»ƒm tra sá»©c khá»e"""
      return jsonify({
          "service": "AdnLog Query API",
          "status": "healthy",
          "spark_status": "active" if spark else "inactive",
          "timestamp": datetime.now().isoformat()
      })

  @app.route("/query", methods=["GET"])
  def query_user_count():
      """Endpoint chÃ­nh Ä‘á»ƒ truy váº¥n"""
      start_time = datetime.now()
      
      try:
          # Láº¥y tham sá»‘
          id_type = request.args.get("id_type")
          target_id = request.args.get("id")
          mode = request.args.get("mode")
          from_date = request.args.get("from")
          to_date = request.args.get("to")
          
          logger.info(f"Nháº­n truy váº¥n: id_type={id_type}, id={target_id}, mode={mode}, from={from_date}, to={to_date}")
          
          # Kiá»ƒm tra tham sá»‘ báº¯t buá»™c
          if not all([id_type, target_id, mode, from_date, to_date]):
              missing = [param for param, value in [
                  ("id_type", id_type), ("id", target_id), ("mode", mode),
                  ("from", from_date), ("to", to_date)
              ] if not value]
              return jsonify({
                  "success": False,
                  "error": f"Thiáº¿u tham sá»‘ báº¯t buá»™c: {', '.join(missing)}",
                  "meta": {
                      "query_time_seconds": 0.001,
                      "timestamp": datetime.now().isoformat()
                  }
              }), 400
          
          # Kiá»ƒm tra id_type
          if id_type not in ["campaignId", "bannerId"]:
              return jsonify({
                  "success": False,
                  "error": "id_type pháº£i lÃ  'campaignId' hoáº·c 'bannerId'",
                  "meta": {
                      "query_time_seconds": 0.001,
                      "timestamp": datetime.now().isoformat()
                  }
              }), 400
          
          # Kiá»ƒm tra mode
          if mode not in ["click", "view"]:
              return jsonify({
                  "success": False,
                  "error": "mode pháº£i lÃ  'click' hoáº·c 'view'",
                  "meta": {
                      "query_time_seconds": 0.001,
                      "timestamp": datetime.now().isoformat()
                  }
              }), 400
          
          # Kiá»ƒm tra Ä‘á»‹nh dáº¡ng ngÃ y
          try:
              datetime.strptime(from_date, '%Y-%m-%d')
              datetime.strptime(to_date, '%Y-%m-%d')
          except ValueError:
              return jsonify({
                  "success": False,
                  "error": "Äá»‹nh dáº¡ng ngÃ y khÃ´ng há»£p lá»‡. Sá»­ dá»¥ng YYYY-MM-DD",
                  "meta": {
                      "query_time_seconds": 0.001,
                      "timestamp": datetime.now().isoformat()
                  }
              }), 400
          
          # Thá»±c hiá»‡n truy váº¥n Spark
          user_count = processor.query_user_count(id_type, target_id, mode, from_date, to_date)
          
          # TÃ­nh thá»i gian pháº£n há»“i
          end_time = datetime.now()
          query_time = (end_time - start_time).total_seconds()
          
          logger.info(f"Truy váº¥n hoÃ n táº¥t trong {query_time:.3f}s, káº¿t quáº£: {user_count} users")
          
          return jsonify({
              "success": True,
              "data": {
                  "id_type": id_type,
                  "id": target_id,
                  "mode": mode,
                  "from_date": from_date,
                  "to_date": to_date,
                  "user_count": user_count
              },
              "meta": {
                  "query_time_seconds": round(query_time, 3),
                  "timestamp": end_time.isoformat()
              }
          })
          
      except Exception as e:
          end_time = datetime.now()
          query_time = (end_time - start_time).total_seconds()
          logger.error(f"Lá»—i truy váº¥n: {str(e)}")
          
          return jsonify({
              "success": False,
              "error": str(e),
              "meta": {
                  "query_time_seconds": round(query_time, 3),
                  "timestamp": end_time.isoformat()
              }
          }), 500

  if __name__ == "__main__":
      print("ğŸš€ Khá»Ÿi Ä‘á»™ng Server Truy Váº¥n AdnLog...")
      if init_app():
          print("âœ… Khá»Ÿi táº¡o thÃ nh cÃ´ng!")
          print("ğŸŒ Server khá»Ÿi Ä‘á»™ng táº¡i http://0.0.0.0:8080")
          print("ğŸ“– TÃ i liá»‡u API: http://localhost:8080/")
          print("â¤ï¸ Kiá»ƒm tra sá»©c khá»e: http://localhost:8080/health")
          app.run(host="0.0.0.0", port=8080, debug=False, threaded=True)
      else:
          print("âŒ Khá»Ÿi táº¡o tháº¥t báº¡i!")
          exit(1)
  EOF
  ```

#### **6.2 Cháº¡y vá»›i dá»¯ liá»‡u máº«u trÆ°á»›c**
```bash
export SPARK_MODE=local
python3 app_hybrid.py
```

#### **6.3 Chuyá»ƒn sang Remote Mode vá»›i HDFS**
```bash
export SPARK_MODE=remote
python3 app_hybrid.py
```

#### **6.4 Táº¡o script sáº£n xuáº¥t**
```bash
cat > start_production_hdfs.sh << 'EOF'
#!/bin/bash
echo "ğŸš€ Khá»Ÿi Ä‘á»™ng AdnLog API vá»›i dá»¯ liá»‡u HDFS..."

# Thiáº¿t láº­p mÃ´i trÆ°á»ng
export SPARK_MODE=remote
export SPARK_HOME=/data/spark-3.4.3
export PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbin
export PYTHONPATH=$SPARK_HOME/python:$SPARK_HOME/python/lib/py4j-0.10.9.7-src.zip:$PYTHONPATH
export PYSPARK_PYTHON=python3
export HADOOP_CONF_DIR=/data/hadoop-3.3.5/etc/hadoop

# Khá»Ÿi Ä‘á»™ng vá»›i Gunicorn
gunicorn --bind 0.0.0.0:8080 --workers 2 --timeout 300 --preload app_hybrid:app
EOF
chmod +x start_production_hdfs.sh
```

#### **6.5 Cáº­p nháº­t `spark_session.py`**
- Sao lÆ°u file gá»‘c:
  ```bash
  cp spark_session.py spark_session.py.backup
  ```
- Táº¡o phiÃªn báº£n má»›i:
  ```bash
  cat > spark_session.py << 'EOF'
  from pyspark.sql import SparkSession
  import os

  def get_spark():
      """Táº¡o SparkSession vá»›i cáº¥u hÃ¬nh tá»‘i Æ°u"""
      mode = os.environ.get('SPARK_MODE', 'remote')

      if mode == 'remote':
          # DÃ¹ng local mode nhÆ°ng vá»›i truy cáº­p HDFS
          spark = SparkSession.builder \
              .appName("AdnLogAPI-Remote") \
              .master("local[*]") \
              .config("spark.hadoop.fs.defaultFS", "hdfs://adt-platform-dev-106-254:8120") \
              .config("spark.sql.adaptive.enabled", "true") \
              .config("spark.sql.adaptive.coalescePartitions.enabled", "true") \
              .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") \
              .config("spark.sql.execution.arrow.pyspark.enabled", "true") \
              .getOrCreate()
      elif mode == 'yarn':
          # Sáº£n xuáº¥t vá»›i YARN
          spark = SparkSession.builder \
              .appName("AdnLogAPI-YARN") \
              .master("yarn") \
              .config("spark.hadoop.fs.defaultFS", "hdfs://adt-platform-dev-106-254:8120") \
              .config("spark.sql.adaptive.enabled", "true") \
              .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") \
              .getOrCreate()
      else:
          # Local mode cho phÃ¡t triá»ƒn
          spark = SparkSession.builder \
              .appName("AdnLogAPI-Local") \
              .master("local[*]") \
              .config("spark.sql.adaptive.enabled", "true") \
              .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") \
              .getOrCreate()

      spark.sparkContext.setLogLevel("WARN")
      return spark
  EOF
  ```

#### **6.6 Cháº¡y vá»›i HDFS**
```bash
export SPARK_MODE=remote
python3 app_hybrid.py
```

---

## âš¡ Chiáº¿n LÆ°á»£c Tá»‘i Æ¯u Hiá»‡u Suáº¥t

### **1. Cache Bá»™ Nhá»›**
```python
# Dá»¯ liá»‡u Ä‘Æ°á»£c cache vÃ o bá»™ nhá»› láº§n Ä‘áº§u
self.df = df_processed.cache()
count = self.df.count()  # KÃ­ch hoáº¡t caching

# CÃ¡c truy váº¥n sau Ä‘Ã¡nh vÃ o bá»™ nhá»›, khÃ´ng Ä‘Ä©a
result = self.df.filter(...).agg(countDistinct("guid"))
```

### **2. Tá»‘i Æ¯u Spark**
```python
# Adaptive Query Execution
"spark.sql.adaptive.enabled": "true"

# Gá»™p cÃ¡c phÃ¢n vÃ¹ng nhá»  
"spark.sql.adaptive.coalescePartitions.enabled": "true"

# Chuáº©n hÃ³a nhanh
"spark.serializer": "org.apache.spark.serializer.KryoSerializer"

# Chuyá»ƒn dá»¯ liá»‡u dá»±a trÃªn Arrow
"spark.sql.execution.arrow.pyspark.enabled": "true"
```

### **3. Tá»‘i Æ¯u Truy Váº¥n**
```python
# Chuá»—i truy váº¥n hiá»‡u quáº£:
result = self.df.filter(col(id_type) == target_id) \
               .filter(col("click_or_view") == is_click) \
               .filter(col("event_date").between(from_date, to_date)) \
               .agg(countDistinct("guid").alias("user_count"))
```

---

## ğŸ”„ Quy TrÃ¬nh Xá»­ LÃ½ YÃªu Cáº§u

### **SÆ¡ Ä‘á»“ Chi Tiáº¿t:**
```
YÃªu cáº§u tá»« Client
     â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    á»¨ng dá»¥ng Flask                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚
â”‚  â”‚ Kiá»ƒm tra        â”‚    â”‚ Kiá»ƒm tra Ä‘á»‹nh   â”‚                â”‚
â”‚  â”‚ Tham sá»‘         â”‚ â†’  â”‚ dáº¡ng ngÃ y       â”‚                â”‚
â”‚  â”‚ (id_type, id,   â”‚    â”‚ (YYYY-MM-DD)    â”‚                â”‚
â”‚  â”‚  mode, dates)   â”‚    â”‚                 â”‚                â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚
â”‚           â†“                       â†“                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚              Thá»±c thi truy váº¥n Spark                   â”‚â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚â”‚
â”‚  â”‚  â”‚   Lá»c       â”‚â†’ â”‚   Lá»c       â”‚â†’ â”‚   Lá»c       â”‚     â”‚â”‚
â”‚  â”‚  â”‚   theo ID   â”‚  â”‚  theo Mode  â”‚  â”‚ theo khoáº£ng â”‚     â”‚â”‚
â”‚  â”‚  â”‚             â”‚  â”‚             â”‚  â”‚   thá»i gian  â”‚     â”‚â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚â”‚
â”‚  â”‚           â†“              â†“              â†“               â”‚â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚â”‚
â”‚  â”‚  â”‚        countDistinct("guid")                        â”‚â”‚â”‚
â”‚  â”‚  â”‚        (Äáº¿m user unique)                            â”‚â”‚â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â”‚           â†“                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚              Äá»‹nh dáº¡ng pháº£n há»“i                        â”‚â”‚
â”‚  â”‚  â€¢ ThÃªm thá»i gian thá»±c thi truy váº¥n                    â”‚â”‚
â”‚  â”‚  â€¢ ThÃªm timestamp                                      â”‚â”‚
â”‚  â”‚  â€¢ Äá»‹nh dáº¡ng JSON pháº£n há»“i                             â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â†“
Pháº£n há»“i JSON Ä‘áº¿n Client
```

### **Chiáº¿n lÆ°á»£c Cache Bá»™ Nhá»›:**
```
Giai Ä‘oáº¡n Khá»Ÿi Ä‘á»™ng:
Dá»¯ liá»‡u HDFS/Máº«u â†’ Spark DataFrame â†’ .cache() â†’ LÆ°u trá»¯ bá»™ nhá»›

Giai Ä‘oáº¡n Truy váº¥n:
Cache bá»™ nhá»› â†’ Thao tÃ¡c lá»c â†’ Tá»•ng há»£p â†’ Káº¿t quáº£
```

**Thá»i gian hiá»‡u suáº¥t**:
- **Khá»Ÿi Ä‘á»™ng**: ~10-15 giÃ¢y (táº£i + cache dá»¯ liá»‡u)
- **Truy váº¥n Ä‘áº§u tiÃªn**: < 1 giÃ¢y (tá»« cache bá»™ nhá»›)
- **CÃ¡c truy váº¥n sau**: < 0.5 giÃ¢y (truy cáº­p cache tá»‘i Æ°u)
- **Äá»“ng thá»i**: 50+ user vá»›i 2 workers

---

## ğŸ› Xá»­ LÃ½ CÃ¡c Váº¥n Äá» ThÆ°á»ng Gáº·p

### **1. Lá»—i Import PySpark**
**Triá»‡u chá»©ng:**
```
ModuleNotFoundError: No module named 'pyspark'
```

**NguyÃªn nhÃ¢n:** PYTHONPATH khÃ´ng Ä‘Æ°á»£c thiáº¿t láº­p Ä‘Ãºng

**Giáº£i phÃ¡p:**
```bash
# Kiá»ƒm tra SPARK_HOME
echo $SPARK_HOME

# Thiáº¿t láº­p PYTHONPATH thá»§ cÃ´ng
export PYTHONPATH=$SPARK_HOME/python:$SPARK_HOME/python/lib/py4j-0.10.9.7-src.zip:$PYTHONPATH

# Kiá»ƒm tra import
python3 -c "from pyspark.sql import SparkSession; print('PySpark OK')"

# Náº¿u váº«n lá»—i, tÃ¬m phiÃªn báº£n py4j Ä‘Ãºng
ls -la $SPARK_HOME/python/lib/py4j*.zip
export PYTHONPATH=$SPARK_HOME/python:$SPARK_HOME/python/lib/py4j-*-src.zip:$PYTHONPATH
```

### **2. Cá»•ng ÄÃ£ ÄÆ°á»£c Sá»­ Dá»¥ng**
**Triá»‡u chá»©ng:**
```
Address already in use
Port 5000 is in use by another program
```

**Giáº£i phÃ¡p (khÃ´ng cáº§n sudo):**
```bash
# TÃ¬m tiáº¿n trÃ¬nh Ä‘ang dÃ¹ng cá»•ng (chá»‰ tiáº¿n trÃ¬nh cá»§a user)
lsof -i :5000

# Káº¿t thÃºc tiáº¿n trÃ¬nh náº¿u lÃ  cá»§a user
kill -9 <PID>

# Hoáº·c dÃ¹ng cá»•ng khÃ¡c (khuyáº¿n nghá»‹)
sed -i 's/port=5000/port=8888/' app.py
python3 app.py

# Server sáº½ cháº¡y trÃªn http://localhost:8888

# Hoáº·c Ä‘áº·t biáº¿n mÃ´i trÆ°á»ng PORT
export PORT=8888
python3 app.py
```

### **3. Spark KhÃ´ng ÄÆ°á»£c TÃ¬m Tháº¥y**
**Triá»‡u chá»©ng:**
```
SPARK_HOME not set or Spark not found
```

**Giáº£i phÃ¡p:**
```bash
# TÃ¬m cÃ i Ä‘áº·t Spark
find /opt /usr/local /data -name "spark-submit" 2>/dev/null

# Thiáº¿t láº­p SPARK_HOME
export SPARK_HOME="/data/spark-3.4.3"  # Thay Ä‘á»•i path phÃ¹ há»£p
echo 'export SPARK_HOME="/data/spark-3.4.3"' >> .env

# Kiá»ƒm tra Spark
$SPARK_HOME/bin/spark-submit --version
```

### **4. Váº¥n Äá» Bá»™ Nhá»›**
**Triá»‡u chá»©ng:**
```
Java heap space error
OutOfMemoryError
```

**Giáº£i phÃ¡p:**
```bash
# Giáº£m dung lÆ°á»£ng bá»™ nhá»› Spark
export SPARK_DRIVER_MEMORY=1g
export SPARK_EXECUTOR_MEMORY=1g

# Hoáº·c dÃ¹ng Ã­t workers
gunicorn --workers 1 wsgi:app

# Kiá»ƒm tra bá»™ nhá»› há»‡ thá»‘ng
free -h
```

### **5. Server Tá»± Dá»«ng**
**Triá»‡u chá»©ng:** Server khá»Ÿi Ä‘á»™ng nhÆ°ng tá»± thoÃ¡t ngay

**CÃ¡c bÆ°á»›c gá»¡ lá»—i:**
```bash
# Cháº¡y mode phÃ¡t triá»ƒn Ä‘á»ƒ xem lá»—i
./start_dev.sh

# Hoáº·c cháº¡y trá»±c tiáº¿p
source .env
python3 app.py

# Kiá»ƒm tra log
tail -f logs/error.log

# Kiá»ƒm tra tá»«ng thÃ nh pháº§n
python3 -c "from pyspark.sql import SparkSession; print('PySpark OK')"
python3 -c "import flask; print('Flask OK')"
python3 -c "from wsgi import application; print('WSGI OK')"
```

### **6. Hiá»‡u Suáº¥t Truy Váº¥n Cháº­m**
**Triá»‡u chá»©ng:** Thá»i gian truy váº¥n > 5 giÃ¢y

**Tá»‘i Æ°u hÃ³a:**
```bash
# Kiá»ƒm tra dá»¯ liá»‡u Ä‘Ã£ Ä‘Æ°á»£c cache chÆ°a
# Trong log pháº£i tháº¥y: "Cached X records successfully"

# TÄƒng Ä‘á»™ song song Spark
export SPARK_SQL_ADAPTIVE_COALESCE_PARTITIONS_ENABLED=true

# Theo dÃµi Spark UI
# http://localhost:4040 (khi server Ä‘ang cháº¡y)
```

### **7. Lá»—i Káº¿t Ná»‘i HDFS**
**Triá»‡u chá»©ng:**
```
Connection refused: hdfs://server:8120
```

**Giáº£i phÃ¡p:**
```bash
# Chuyá»ƒn sang local mode
export SPARK_MODE=local
python3 app.py

# Hoáº·c kiá»ƒm tra káº¿t ná»‘i HDFS
hdfs dfs -ls /data/Parquet/AdnLog/
```

---

## ğŸ“Š TÃ i Liá»‡u Endpoint API

### **GET /health**
```json
{
  "status": "healthy",
  "service": "AdnLog Query API", 
  "spark_status": "active",
  "timestamp": "2025-07-27T15:53:46.481000"
}
```

### **GET /query**
**Tham sá»‘:**
- `id_type`: `campaignId` hoáº·c `bannerId`
- `id`: GiÃ¡ trá»‹ ID má»¥c tiÃªu
- `mode`: `click` hoáº·c `view`
- `from`: NgÃ y báº¯t Ä‘áº§u (YYYY-MM-DD)
- `to`: NgÃ y káº¿t thÃºc (YYYY-MM-DD)

**Pháº£n há»“i:**
```json
{
  "success": true,
  "data": {
    "user_count": 3,
    "id_type": "campaignId",
    "id": "12345",
    "mode": "view",
    "from_date": "2024-07-03",
    "to_date": "2024-07-05"
  },
  "meta": {
    "query_time_seconds": 0.309,
    "timestamp": "2025-07-27T15:53:46.481000"
  }
}
```

---

## ğŸ¯ Káº¿t Quáº£ Äáº¡t ÄÆ°á»£c

### **Chá»‰ sá»‘ Hiá»‡u suáº¥t:**
- âœ… **Thá»i gian pháº£n há»“i truy váº¥n**: < 1 giÃ¢y (yÃªu cáº§u < 1 phÃºt)
- âœ… **Sá»‘ user Ä‘á»“ng thá»i**: 50+ user
- âœ… **Luá»“ng xá»­ lÃ½**: 100+ yÃªu cáº§u/phÃºt
- âœ… **Dung lÆ°á»£ng bá»™ nhá»›**: ~500MB má»—i worker
- âœ… **Thá»i gian khá»Ÿi Ä‘á»™ng**: ~15 giÃ¢y

### **TÃ­nh nÄƒng:**
- âœ… **Nhiá»u mÃ´i trÆ°á»ng**: Há»— trá»£ Local/YARN/Remote
- âœ… **Triá»ƒn khai tá»± Ä‘á»™ng**: 3 bÆ°á»›c triá»ƒn khai (giáº£i nÃ©n â†’ thiáº¿t láº­p â†’ khá»Ÿi Ä‘á»™ng)
- âœ… **Kiá»ƒm tra toÃ n diá»‡n**: Bá»™ kiá»ƒm tra tá»± Ä‘á»™ng
- âœ… **Sáºµn sÃ ng sáº£n xuáº¥t**: Gunicorn + logging + giÃ¡m sÃ¡t
- âœ… **Xá»­ lÃ½ lá»—i**: Kiá»ƒm tra + pháº£n há»“i lá»—i há»£p lÃ½

### **Kháº£ nÄƒng má»Ÿ rá»™ng:**
- âœ… **Má»Ÿ rá»™ng ngang**: Nhiá»u worker Gunicorn
- âœ… **Má»Ÿ rá»™ng dá»c**: Cáº¥u hÃ¬nh tÃ i nguyÃªn Spark
- âœ… **Má»Ÿ rá»™ng dá»¯ liá»‡u**: TÃ­ch há»£p HDFS cho dá»¯ liá»‡u lá»›n

---

## ğŸ† Äá»•i Má»›i ChÃ­nh
**Thay vÃ¬ truy váº¥n trá»±c tiáº¿p tá»« HDFS má»—i yÃªu cáº§u (cháº­m), tÃ´i preload vÃ  cache toÃ n bá»™ dá»¯ liá»‡u vÃ o bá»™ nhá»› Spark, biáº¿n I/O Ä‘Ä©a thÃ nh truy cáº­p bá»™ nhá»› - Ä‘Ã¢y lÃ  lÃ½ do chÃ­nh giÃºp Ä‘áº¡t hiá»‡u suáº¥t yÃªu cáº§u tá»« phÃºt xuá»‘ng giÃ¢y.**

**ğŸ‰ Káº¿t quáº£: API sáºµn sÃ ng sáº£n xuáº¥t vá»›i triá»ƒn khai chá»‰ 3 lá»‡nh, hoÃ n toÃ n Ä‘Ã¡p á»©ng yÃªu cáº§u bÃ i toÃ¡n!**

---

## ğŸ“¦ Cáº¥u trÃºc GÃ³i: adnlog-api-complete.zip
```
adnlog-api-complete/
â”œâ”€â”€ ğŸ”§ Core Application
â”‚   â”œâ”€â”€ app.py                 # á»¨ng dá»¥ng Flask chÃ­nh
â”‚   â”œâ”€â”€ spark_session.py       # Cáº¥u hÃ¬nh Spark
â”‚   â”œâ”€â”€ data_processor.py      # Logic xá»­ lÃ½ dá»¯ liá»‡u
â”‚   â””â”€â”€ wsgi.py               # Äiá»ƒm nháº­p WSGI
â”œâ”€â”€ ğŸš€ Deployment Scripts
â”‚   â”œâ”€â”€ setup_environment.sh   # Script thiáº¿t láº­p tá»± Ä‘á»™ng
â”‚   â”œâ”€â”€ start_server.sh       # Server sáº£n xuáº¥t
â”‚   â”œâ”€â”€ start_dev.sh          # Server phÃ¡t triá»ƒn
â”‚   â””â”€â”€ test_api.sh           # Bá»™ kiá»ƒm tra API
â”œâ”€â”€ ğŸ“‹ Configuration
â”‚   â”œâ”€â”€ requirements.txt      # Phá»¥ thuá»™c Python
â”‚   â””â”€â”€ README.md            # TÃ i liá»‡u
â””â”€â”€ ğŸ“– Documentation
    â””â”€â”€ deploy_instructions.md # HÆ°á»›ng dáº«n triá»ƒn khai
```

---

## ğŸ“ Ghi ChÃº
- Äáº£m báº£o cÃ¡c Ä‘Æ°á»ng dáº«n (vÃ­ dá»¥: `/data/spark-3.4.3`, `/data/hadoop-3.3.5/etc/hadoop`) khá»›p vá»›i cáº¥u hÃ¬nh server.
- Náº¿u xáº£y ra lá»—i, kiá»ƒm tra log vÃ  sá»­ dá»¥ng cÃ¡c bÆ°á»›c gá»¡ lá»—i Ä‘Æ°á»£c cung cáº¥p.
- Thá»i gian hiá»‡n táº¡i lÃ  19:41 PM +07, hÃ£y lÃªn káº¿ hoáº¡ch báº£o trÃ¬ server phÃ¹ há»£p.
## ğŸ† **Key Innovation**

**Thay vÃ¬ query trá»±c tiáº¿p tá»« HDFS má»—i request (cháº­m), tÃ´i pre-load vÃ  cache toÃ n bá»™ data vÃ o Spark memory, biáº¿n disk I/O thÃ nh memory access - Ä‘Ã¢y lÃ  lÃ½ do chÃ­nh giÃºp Ä‘áº¡t Ä‘Æ°á»£c performance yÃªu cáº§u tá»« phÃºt xuá»‘ng giÃ¢y.**

**ğŸ‰ Káº¿t quáº£: API production-ready vá»›i deployment chá»‰ 3 lá»‡nh, hoÃ n toÃ n Ä‘Ã¡p á»©ng yÃªu cáº§u bÃ i toÃ¡n!**
##  Káº¾T QUáº¢ CHáº Y ÄÆ¯á»¢C 
SERVER CHáº Y 
<img width="1663" height="747" alt="image" src="https://github.com/user-attachments/assets/c71454df-f775-4280-b129-51ec12fbc5fc" />
<img width="1127" height="312" alt="image" src="https://github.com/user-attachments/assets/e730cf72-1ff5-4ceb-a55e-92fb83fc7f5d" />
## Káº¾T QUáº¢ Äáº T ÄÆ¯á»¢C 
<img width="1904" height="431" alt="image" src="https://github.com/user-attachments/assets/995883b6-3a15-46dd-ad7e-650af111d599" />
